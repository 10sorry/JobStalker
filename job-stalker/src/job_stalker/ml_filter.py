import asyncio
import re
import os
import logging
import json
from datetime import datetime
from typing import Dict, Optional, Callable, AsyncGenerator, List
import aiohttp
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from dataclasses import dataclass, field

log = logging.getLogger("ml_filter")
console = Console()

# URL –¥–ª—è Ollama API (–ª–æ–∫–∞–ª—å–Ω—ã–π)
OLLAMA_API_URL = "http://localhost:11434/api/generate"

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º API –∫–ª—é—á –∏–∑ config
try:
    from .config import GEMINI_API_KEY
except ImportError:
    GEMINI_API_KEY = None

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ
RESUME_DATA: Optional[Dict] = None

# Output directory –¥–ª—è PDF
OUTPUT_DIR = "./output"


@dataclass
class ResumeComparison:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Ä–µ–∑—é–º–µ"""
    match_score: int = 0
    strong_sides: List[str] = field(default_factory=list)
    weak_sides: List[str] = field(default_factory=list)
    missing_skills: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    improved_resume: str = ""
    cover_letter_hint: str = ""

@dataclass
class VacancyAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–∏"""
    suitable: bool
    analysis: str = ""
    comparison: Optional[ResumeComparison] = None
    improved_resume_path: Optional[str] = None
    
    def __bool__(self):
        return self.suitable



# –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è UI (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –µ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å)
DEFAULT_FILTER_PROMPT_EXAMPLE = """–ò—â—É –ø–æ–∑–∏—Ü–∏–∏:
‚úÖ Unreal Engine —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ (junior, junior+, middle)
‚úÖ C++ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –≤ –≥–µ–π–º–¥–µ–≤–µ
‚úÖ Game programmer
‚úÖ Technical Artist —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º

–ù–ï –ø–æ–¥—Ö–æ–¥—è—Ç:
‚ùå Senior –ø–æ–∑–∏—Ü–∏–∏ (3+ –≥–æ–¥–∞ –æ–ø—ã—Ç–∞)
‚ùå Unity-only –±–µ–∑ Unreal
‚ùå –ú–µ–Ω–µ–¥–∂–µ—Ä—ã, HR, –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∏
‚ùå –•—É–¥–æ–∂–Ω–∏–∫–∏ –±–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
‚ùå QA –±–µ–∑ –∫–æ–¥–∞"""


def get_filter_prompt(custom_prompt: str = "", resume_summary: str = "") -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–π.
    
    –õ–æ–≥–∏–∫–∞:
    - –ï—Å–ª–∏ custom_prompt –ø—É—Å—Ç–æ–π -> –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ–¥—Ö–æ–¥—è—Ç (suitable: true)
    - –ï—Å–ª–∏ custom_prompt –∑–∞–ø–æ–ª–Ω–µ–Ω -> LLM —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º
    """
    
    # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –ø—É—Å—Ç–æ–π - –Ω–µ –Ω—É–∂–µ–Ω LLM –∞–Ω–∞–ª–∏–∑, –≤—Å–µ –ø–æ–¥—Ö–æ–¥—è—Ç
    if not custom_prompt or not custom_prompt.strip():
        return ""
    
    # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    base = f"""–¢—ã —Ñ–∏–ª—å—Ç—Ä –≤–∞–∫–∞–Ω—Å–∏–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–∞–∫–∞–Ω—Å–∏—é –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ö–†–ò–¢–ï–†–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{custom_prompt.strip()}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON (–±–µ–∑ markdown, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤):
{{
  "suitable": true –∏–ª–∏ false,
  "reasons_fit": ["–ø–æ—á–µ–º—É –ø–æ–¥—Ö–æ–¥–∏—Ç"],
  "reasons_reject": ["–ø–æ—á–µ–º—É –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç"],
  "position_type": "developer/manager/designer/artist/qa/other",
  "summary": "–∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"
}}"""

    if resume_summary:
        base = f"–†–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: {resume_summary}\n\n{base}"
    
    return base


# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def get_default_prompt(resume_summary: str = "") -> str:
    return get_filter_prompt(DEFAULT_FILTER_PROMPT_EXAMPLE, resume_summary)


def get_comparison_prompt(vacancy_text: str, resume_text: str) -> str:
    """–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Ä–µ–∑—é–º–µ"""
    return f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ IT-—Ä–µ–∫—Ä—É—Ç–∏–Ω–≥—É. –°—Ä–∞–≤–Ω–∏ –≤–∞–∫–∞–Ω—Å–∏—é —Å —Ä–µ–∑—é–º–µ –∏ —Å–æ–∑–¥–∞–π –£–õ–£–ß–®–ï–ù–ù–û–ï —Ä–µ–∑—é–º–µ.

–í–ê–ö–ê–ù–°–ò–Ø:
{vacancy_text[:2000]}

–¢–ï–ö–£–©–ï–ï –†–ï–ó–Æ–ú–ï:
{resume_text[:2000]}

–í–ê–ñ–ù–û: –í –ø–æ–ª–µ "improved_resume" –Ω–∞–ø–∏—à–∏ –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ (–Ω–µ –∫—Ä–∞—Ç–∫–∏–π, –∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç), –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥ —ç—Ç—É –≤–∞–∫–∞–Ω—Å–∏—é. –î–æ–±–∞–≤—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏, –ø–æ–¥—á–µ—Ä–∫–Ω–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ–ø—ã—Ç.

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON (–±–µ–∑ markdown):
{{
  "match_score": —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 100,
  "strong_sides": ["—Å–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 1", "—Å–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 2"],
  "weak_sides": ["—Å–ª–∞–±–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 1"],
  "missing_skills": ["–Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –Ω–∞–≤—ã–∫ 1", "–Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –Ω–∞–≤—ã–∫ 2"],
  "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2"],
  "cover_letter_hint": "–ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞",
  "improved_resume": "–ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ –∑–¥–µ—Å—å, –º–∏–Ω–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤"
}}

JSON:"""



_stream_callback: Optional[Callable] = None

def set_stream_callback(callback: Optional[Callable]):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç callback –¥–ª—è streaming"""
    global _stream_callback
    _stream_callback = callback

async def notify_stream(chunk: str, stream_type: str = "analysis"):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫ —á–µ—Ä–µ–∑ callback"""
    if _stream_callback:
        try:
            await _stream_callback({
                "type": "stream",
                "stream_type": stream_type,
                "chunk": chunk
            })
        except Exception as e:
            log.warning(f"Stream callback error: {e}")

async def ollama_stream(prompt: str, model: str = "mistral7") -> AsyncGenerator[str, None]:
    """Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama API"""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                OLLAMA_API_URL, 
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                if response.status != 200:
                    error = await response.text()
                    log.error(f"Ollama API error: {error}")
                    yield f"[ERROR: {response.status}]"
                    return
                
                async for line in response.content:
                    if line:
                        try:
                            data = json.loads(line.decode('utf-8'))
                            chunk = data.get('response', '')
                            if chunk:
                                yield chunk
                            if data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue
    except aiohttp.ClientError as e:
        log.error(f"Ollama connection error: {e}")
        yield f"[ERROR: {e}]"
    except Exception as e:
        log.error(f"Ollama stream error: {e}")
        yield f"[ERROR: {e}]"

async def ollama_generate(prompt: str, model: str = "mistral7", stream_type: str = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º streaming"""
    full_response = ""
    
    if stream_type:
        await notify_stream("[START]", stream_type)
    
    async for chunk in ollama_stream(prompt, model):
        full_response += chunk
        if stream_type:
            await notify_stream(chunk, stream_type)
            await asyncio.sleep(0.005)  # –£–º–µ–Ω—å—à–∏–ª –∑–∞–¥–µ—Ä–∂–∫—É
    
    if stream_type:
        await notify_stream("[END]", stream_type)
    
    return full_response



def extract_json_safely(text: str) -> dict:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ AI"""
    text = text.replace('```json', '').replace('```', '').strip()
    
    # –ú–µ—Ç–æ–¥ 1: –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏
    depth = 0
    start = -1
    for i, char in enumerate(text):
        if char == '{':
            if depth == 0:
                start = i
            depth += 1
        elif char == '}':
            depth -= 1
            if depth == 0 and start != -1:
                try:
                    return json.loads(text[start:i+1])
                except json.JSONDecodeError:
                    continue
    
    # –ú–µ—Ç–æ–¥ 2: Regex
    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(0))
        except:
            pass
    
    # –ú–µ—Ç–æ–¥ 3: Greedy
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(0))
        except:
            pass
    
    return {}



def normalize_resume_data(data: dict) -> dict:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ"""
    normalized = {}
    
    # Experience
    if 'experience_years' in data:
        normalized['experience_years'] = data['experience_years']
    elif 'experience' in data:
        exp = data['experience']
        if isinstance(exp, list):
            normalized['experience_years'] = len(exp) * 2
            projects = []
            for item in exp:
                if isinstance(item, dict):
                    company = item.get('company', item.get('project', ''))
                    position = item.get('positionTitle', item.get('position', ''))
                    if company or position:
                        projects.append(f"{position} @ {company}".strip(' @'))
            if projects:
                normalized['projects'] = projects
        elif isinstance(exp, (int, float)):
            normalized['experience_years'] = exp
    
    # Level
    if 'level' in data:
        normalized['level'] = data['level']
    else:
        years = normalized.get('experience_years', 0)
        if isinstance(years, (int, float)):
            normalized['level'] = 'junior' if years <= 2 else 'middle' if years <= 5 else 'senior'
    
    # Skills
    skills = []
    if 'key_skills' in data:
        skills = data['key_skills']
    elif 'skills' in data:
        sk = data['skills']
        if isinstance(sk, list):
            skills = sk
        elif isinstance(sk, dict):
            for items in sk.values():
                if isinstance(items, list):
                    skills.extend(items)
    normalized['key_skills'] = skills[:10]
    
    if 'projects' not in normalized:
        normalized['projects'] = data.get('projects', [])
    normalized['summary'] = data.get('summary', '')
    if 'name' in data:
        normalized['name'] = data['name']
    
    return normalized


async def load_resume(file_path: str, model_type: str = "mistral") -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ"""
    global RESUME_DATA
    
    log.info(f"üöÄ load_resume: model={model_type}, file={file_path}")
    
    if not os.path.exists(file_path):
        return {"error": "File not found"}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            resume_text = f.read()
        log.info(f"üìÑ Resume: {len(resume_text)} chars")
    except Exception as e:
        return {"error": f"Read error: {e}"}
    
    prompt = f"""–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—é–º–µ. –í–µ—Ä–Ω–∏ JSON:
{{
  "experience_years": —á–∏—Å–ª–æ,
  "level": "junior"/"middle"/"senior",
  "key_skills": ["–Ω–∞–≤—ã–∫1", "–Ω–∞–≤—ã–∫2"],
  "projects": ["–ø—Ä–æ–µ–∫—Ç1"],
  "summary": "–∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä"
}}

–†–µ–∑—é–º–µ:
{resume_text[:2000]}

JSON:"""
    
    try:
        if model_type == "gemini" and GEMINI_API_KEY:
            output = await _call_gemini(prompt)
        elif model_type == "mistral":
            output = await ollama_generate(prompt, "mistral7", "resume_analysis")
        else:
            output = await ollama_generate(prompt, "llama3.2:3b", "resume_analysis")
        
        raw_data = extract_json_safely(output)
        RESUME_DATA = normalize_resume_data(raw_data)
        RESUME_DATA['raw_text'] = resume_text
        
        log.info(f"‚úÖ Resume: level={RESUME_DATA.get('level')}, exp={RESUME_DATA.get('experience_years')}")
        
        return RESUME_DATA
        
    except Exception as e:
        log.error(f"Resume analysis error: {e}")
        return {"error": str(e)}


async def _call_gemini(prompt: str) -> str:
    """–í—ã–∑–æ–≤ Gemini API"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.3, "maxOutputTokens": 1000}
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status != 200:
                raise Exception(f"Gemini API error {response.status}")
            data = await response.json()
    
    return data['candidates'][0]['content']['parts'][0]['text'].strip()



async def compare_with_resume(vacancy_text: str, vacancy_title: str = "") -> ResumeComparison:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Ä–µ–∑—é–º–µ - –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –û–¢–î–ï–õ–¨–ù–û –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    if not RESUME_DATA or 'raw_text' not in RESUME_DATA:
        log.warning("No resume for comparison")
        return ResumeComparison()
    
    resume_text = RESUME_DATA['raw_text']
    log.info(f"üìù Resume text length: {len(resume_text)}")
    log.info(f"üìù Vacancy text length: {len(vacancy_text)}")
    
    prompt = get_comparison_prompt(vacancy_text, resume_text)
    
    log.info("üîÑ Comparing vacancy with resume...")
    
    try:
        output = await ollama_generate(prompt, "mistral7", "comparison")
        log.info(f"üìù ML output length: {len(output)}")
        
        data = extract_json_safely(output)
        log.info(f"üìù Parsed data keys: {list(data.keys())}")
        log.info(f"üìù improved_resume length: {len(data.get('improved_resume', ''))}")
        
        result = ResumeComparison(
            match_score=data.get('match_score', 0),
            strong_sides=data.get('strong_sides', []),
            weak_sides=data.get('weak_sides', []),
            missing_skills=data.get('missing_skills', []),
            recommendations=data.get('recommendations', []),
            improved_resume=data.get('improved_resume', ''),
            cover_letter_hint=data.get('cover_letter_hint', '')
        )
        
        log.info(f"‚úÖ Comparison done: score={result.match_score}, improved_len={len(result.improved_resume)}")
        return result
        
    except Exception as e:
        log.error(f"Comparison error: {e}")
        return ResumeComparison()



async def analyze_vacancy(text: str, model_type: str = "mistral") -> VacancyAnalysis:
    """
    –ê–Ω–∞–ª–∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏ - –ë–ï–ó comparison.
    
    –õ–æ–≥–∏–∫–∞:
    - –ï—Å–ª–∏ custom_prompt –ø—É—Å—Ç–æ–π -> –≤–∞–∫–∞–Ω—Å–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Ö–æ–¥–∏—Ç
    - –ï—Å–ª–∏ custom_prompt –∑–∞–ø–æ–ª–Ω–µ–Ω -> LLM —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º
    """
    if len(text.strip()) < 20:
        return VacancyAnalysis(False, "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
    
    try:
        from .web_ui import get_current_settings
        settings = get_current_settings()
        custom_prompt = settings.get("custom_prompt", "")
        resume_summary = settings.get("resume_summary", "")
    except:
        custom_prompt = ""
        resume_summary = ""
    
    # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –µ—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –ø—É—Å—Ç–æ–π - –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ–¥—Ö–æ–¥—è—Ç
    if not custom_prompt or not custom_prompt.strip():
        log.info("üìã Filter prompt empty - vacancy auto-approved")
        return VacancyAnalysis(
            suitable=True,
            analysis="‚úÖ –§–∏–ª—å—Ç—Ä –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω ‚Äî –≤–∞–∫–∞–Ω—Å–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.\n\nüí° –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤ Settings ‚Üí Search Filter Prompt",
            comparison=None
        )
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    filter_prompt = get_filter_prompt(custom_prompt, resume_summary)
    full_prompt = f"{filter_prompt}\n\n–í–ê–ö–ê–ù–°–ò–Ø:\n{text.strip()[:1500]}\n\nJSON:"
    
    log.info(f"üìä Analyzing with {model_type.upper()}...")
    
    try:
        if model_type == "mistral":
            output = await ollama_generate(full_prompt, "mistral7")
        elif model_type == "gemini" and GEMINI_API_KEY:
            output = await _call_gemini(full_prompt)
        else:
            output = await ollama_generate(full_prompt, "llama3.2:3b")
        
        data = extract_json_safely(output)
        
        suitable = data.get('suitable', False)
        if isinstance(suitable, str):
            suitable = suitable.lower() in ('true', 'yes', '–¥–∞', '1')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ position_type (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        position_type = data.get('position_type', '').lower()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑
        analysis_parts = []
        
        reasons_fit = data.get('reasons_fit', [])
        reasons_reject = data.get('reasons_reject', data.get('reasons_lack', []))
        summary = data.get('summary', '')
        
        if isinstance(reasons_fit, str):
            reasons_fit = [reasons_fit]
        if isinstance(reasons_reject, str):
            reasons_reject = [reasons_reject]
        
        if reasons_fit:
            analysis_parts.append("‚úÖ **–ü–æ–¥—Ö–æ–¥–∏—Ç:**\n" + "\n".join(f"  ‚Ä¢ {r}" for r in reasons_fit))
        if reasons_reject:
            analysis_parts.append("‚ùå **–ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç:**\n" + "\n".join(f"  ‚Ä¢ {r}" for r in reasons_reject))
        if summary:
            analysis_parts.append(f"üìã **–í—ã–≤–æ–¥:** {summary}")
        if position_type:
            analysis_parts.append(f"üè∑Ô∏è **–¢–∏–ø:** {position_type}")
        
        analysis_text = "\n\n".join(analysis_parts) if analysis_parts else output[:500]
        
        return VacancyAnalysis(
            suitable=suitable,
            analysis=analysis_text,
            comparison=None
        )
        
    except Exception as e:
        log.error(f"Vacancy analysis error: {e}")
        return VacancyAnalysis(False, f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")


async def ml_interesting_async(text: str) -> VacancyAnalysis:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        from .web_ui import get_current_settings
        settings = get_current_settings()
        model_type = settings.get("model_type", "mistral")
    except:
        model_type = "mistral"
    
    return await analyze_vacancy(text, model_type)



SESSION_FILE = "./data/session.json"

def save_session():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ—Å—Å–∏—é"""
    if RESUME_DATA:
        os.makedirs("./data", exist_ok=True)
        session = {
            "resume_data": {k: v for k, v in RESUME_DATA.items() if k != '_original'},
            "saved_at": datetime.now().isoformat()
        }
        try:
            with open(SESSION_FILE, 'w', encoding='utf-8') as f:
                json.dump(session, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(f"Session save error: {e}")

def load_session():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–µ—Å—Å–∏—é"""
    global RESUME_DATA
    try:
        if os.path.exists(SESSION_FILE):
            with open(SESSION_FILE, 'r', encoding='utf-8') as f:
                session = json.load(f)
            RESUME_DATA = session.get('resume_data')
            if RESUME_DATA:
                log.info(f"üìÇ Session loaded")
                return True
    except Exception as e:
        log.error(f"Session load error: {e}")
    return False

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
load_session()
